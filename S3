 Simple Storage System: Store data securily

 S3 is object based storage - means we can store data like files,pdf,pictures and vidoes. S3 is not a Block Based Storage - means it cannot store databases, computing systems.

  Files size can be from 0 to 5TB.

 S3 has Buckets where files will be stored. These Buckets are universal and should have unique names because:
   .S3 buckets are used as domains. As we cannot use the same domain created by others our bucket name should be unique from others.

 Bucket url will be like : https://s3-eu-west-1.amazonaws.com/yogitest
   here s3 is resource name and yogitest is bucketname

  when a upload to bucket is successful - 200 https code is generated


Data Consisteny in S3:
1) Read and Write consistency for new objects - if we PUT a new file in S3, it will be avilable in same way across globe.
2) Eventual Consistency for PUTs and DELETEs - if we modify/delete a file in S3, it might some time in replicating that updated information across globe.


S3 is a Key-Value Store:
a) Key : Name of the file/object uploaded
b) Value : Data of file uploaded. Sequence of bytes.
c) Version ID : version of data
d) Metadata : Data about data , like what kind of data and parameters
e) Subresources:
   1) ACL (Access Control Lists) : Policy/permissions attached to individual files in Bucket.
      ** Remeber ACL is not json based.
   2) Torrent

Basics:

S3 is 99.99% available - means always available
S3 is 99.99999999999% ( 11 9's) durtable - less likeley to loss data we upoloaded
Tiered Storage Available : Different storage classes available
Lifecycle managemnet : Movement of data from one storage class to other depending on time since uploaded.
Versioning : Versioning of data uploaded , so we can retireve data as per version.
Encryption : Encrytping data uploaded
Secure data using Access : IAM and ACL's and Bucket Policies (policies created at bucket level)



Bucket:

1) Buckets are universal and should have unique names because (name should be 3 to 63 characters, no uppercase and underscore):
   .S3 buckets are used as domains. As we cannot use the same domain created by others our bucket name should be unique from others.

2) Objects stored in a Bucket will remain only the region of Bucket created, until unless moved to cross region

3) Bucket creation using API : first create client. If client is created normally then bucket can be created in client region or any specified region.
                            If client created at a specified location, then bucket should only be created in that region.

4) Bucket url will be like : 
. Virtual hosted url : http://bucket.s3.amazonaws.com
. Path style url : http://s3.amazonaws.com/bucket

5) If you just want to empty the bucket and not delete it, make sure you remove the lifecycle configuration rule you added to empty the bucket so that any new objects you create in the bucket will remain in the bucket.

6) Encryption:
Server side encryption can use amaon provided keys (AWS - S3) or KMS
Data will be encrypted before uploading it to bucket and decrypted during download.
Encryption with cross Region Replication: 
a) If source bucket has no encryoption but target bucket has. Then data will be encrypted at target bucket with it's policies.
b) If source bucket has encryption set, then data at target bucket will also have same encryption as source bucket.

7) Transfer Acceleration: a) Helps speed up upload of data from client to S3 bucket.
   b) Uses AWS cloudfront edge location concept
   c) can result in extra charge
   d) some use cases:
       a1) when multiple ppl upload data to one bucket from across world
       a2) upload GB to TB's of data to longer AWS S3 region
   e) When Transfer acceleration is enabled we get a domain name (new url) to use to and fro data trabnsfer of S3. We use this url to upload data to edge location and then edge location uploads data to S3.

8) Requestor pays bucket : Instead of uploader , downloader of data will pay. Anonymous access is not allowed over here.



Object:

1) Key : Name of object/file
2) Valuye : Content in key/object/file which will be in bytes.

3) S3 Storage Classes:
   Frequently Accessed Data:

   a) S3 standard : For frequently accessed data .99.99% available and 99.999--- durable ( Highly stable and can recover 2 AZ's disasters)
   b) Reduced Redendency : For frequently accessed data. Stores reproducable data. DONOT USE IT

   Infrequently accessed data:

   c) S3 - IA (Infrequent ACcess) : For data accessed less frequently but requires rapid availability when required
                                 charged less than s3
                                 charged for retrvel process
   d) S3 One Zone - IA : less frequentky accessed data and doesn't require resillence 
                       less cost than s3 and s3-ia

   e) Glacier: very cheap
            used for archival data
           ** we cannot specify Glacier as storage class of an object through UI. It should be done by lifecycle management.
            Has three archival process:
            a) Expidited - a bit pricy
            b) Standard: data retrival will take 3 - 5 hrs
            c) Bulk


Charges:
   
There is no retrival fee for S3 standard. Retrival Fee is charged for s3-ia and glacier as per GB retrived.

S3 is charged for :
a) Storage : per GB
b) Requests : https requests to s3
c) Storage Management : Tags and metadata
d) Data Transfer , Cross region Replication
e) Transfer Acceleration: Fast , secure data transfer from local to S3. Uses edge locations and cloudfront technology to transfer data from user to main server


**** Bucket side and Object side functions:
Bucket : version, tag,metadata,encryption,requester pay,Transfer accelaration, life cycle policy, cross region replication
Object: Storage class, tag, metada, encryption

Encryption: Encryption shoudl be done as two types:
1) Transit (data movement to and from S3) : It can be done using SSL or client side encryption
2) Rest (while data stored in S3) : It can be done :
   a) Client Side: We encrypt data in local and then upload
   b) Server side : Encrypt data in AWS. We can set this at bucket level and also at Object level, so when something uploads it automatically gets encrypted. Three ways of encryption:
                  1) Server Side Encryption Amazon S3 Managed Keys (SSE - S3) : Data encrypted using keys and MFA by AWS and again those keys are encrypted and rotated. AWS provides keys.
                  2) Server Side Encryption with KMS (SSE-KMS) : Same as AWS-S3 with additional benefits and costs. Employee keys encrypted, audit logs will be provided. Can use either aws keys or customer keys.
                  3) Server Side Encryption with customer provided keys (SSE-C) : Customers provide keys and AWS manages encryption.

Versioning : Versioning can be enabled at Bucket level.
             Once enabled , it cannot be deleted. It can only be suspended.
             It can increase costs as every version of file will stored as a separate file.
             Stores all versions of objects even if it is deleted.
             Integrates with Lifecycle rules
             Can integrate MFA device with versioning to enable more security for changing version state and deletion of object.
             All objects will have version ID by default. It will be set to 'null' if versioning is not enabled.
             If an object is deleted in versioning, it's actually not deleted. The lastest object version is set to a delete marker.
               - So the older version of object still exists in bucket and can retrieved back. To delete a file delete all versions and delete marker.
             Non concurrent expiration policy can be set.

Life Cycle Management: Used for better mangement of data. If data upoloaded becomes old/needs to be deleted we can set time line using life cycle management
  .From standrad you can send to any class
  .From standard to standard IA and one zone IA:
    objects less than 128kb cannot be transferred to IA
    objects must be stored for atleast 30 days in current state before sending to IA
  . From standrad IA to onze zone: objects must be stored for atleast 30 days in current state before sending to onezone
  . From any class to Glacier
  . Glacier to other, onezone IA to IA, IA to standrad are not possible. Any class to Reduced Redundency is also not possible.
  . Can be used either with/ without versioning. Can be applied to both current and previous versions.

Access:
1) Access Control List (ACL) : can be given at bucket level or at object level. Resource Bases Policy and not json format
                               uses canonical name/email address
                               - can provide basic read/write access to other aws accounts
                               - cannot grant permissions to users in account
                               - cannot grant temporary access
                               - cannot deny access
2) Bucket Policy : These are bucket wide and applied for all objects in a bucket.

Cross Region Replication: 

. This is a bucket level feature.

. ***** For cross region replication to work versioning should be enabled on both buckets

. For replication source and destination buckets must be in different AWS regions.

. Replication can be only set from one source bucket to one destination bucket

. Only new or changed objects will be replicated in new bucket and not existing objects. That means if a bucket already have files and then you turn cross region, then those files will NOT be available in replicated bucket.
  Only new files added to bucket or changes made to existing files will be replicated.

. ** If we delete a file in Source bucket (with version enabled), then that file will be also be deleted in Destination bucket. Remember as this is version enabled, a delete marker is put to file in both buckets.
    Now if we delete delete marker from Source bucket, it only deletes it in source bucket. Destination bucket will still have delete marker available.

. Delete Markers will be replacted. But Deleting delete markers and deleting versions of file will NOT be replicated.

. ACL Permissions will also be replicated from source to destination

. We can replicate entire S3 bucket or certain objects only by specifying its key.

. Cross region Replication Monitor : Monitor CRR and produce notifications when replication fails. Uses cloudtrial, cloudlog, SNS, SQS, Lambda, Kinesis and DynamoDB


Cloud Front and Content Delivery Netwrok (CDN) :

. CDN is a collection of servers (network) that delivers web content to users across world.

. Edge Location : This is the location where content will be cached. This is different from Aws Zone (AZ).
                  - Edge location caches data for certain specified time period called TTL (Time To Leave). TTL will always be in seconds.24 hrs i default TTL.
                  - We can also PUT data into edge location. So it not only caches data from other websites,we can also store our own data.
                  - we can clear cached data from edge locations, but will be charged.

. Origin : Origin of our files in aws (In which resource our files are in aws) . 
           - ** It can be S3, ec2, load balancer, route 53.
           - Origin can be AWS resource or any non-aws resource (say github/dropbox)
           - ***** there can be multiple origins to a distribution and they are identified by their origin ID's

. Distribution : It is the name given to CDN. Collection of edge locations and it is of two types:
                 - web distribution: distribution for websites
                 - RTMP : distribution for media content like videos, images.

. ** When a user modified data by PUT/DELETE to distribution. It will first change data at edge location and then update origin.

. ******* Restrict User Access : We can set restrictions on who can access data at CDN.Users should use cloudfront signed url or signed cookies.

. Restrictions can also be applied to countries - like who can access CDn and who cannot.

. We can also set CNAMES - default names to cloudfront url's.

. We can also specify a path in bucket to be made available for CDN instead of complete bucket.


** Storage Gateway:

. It is application which connects on-premise (local) software appliance to AWS cloud storage.

. Gateway is a virual appliance which we install in our data center. Then this gateway will upload data from local to aws.

. Four types of Gateways:
  
  - File Gateway (NFS) : For storing Flat files in S3 . Files like txt, pdf, doc, png and videos.
                         Data accessed thorugh NFS gatepoint
                         As data is in S3, bucket policies, versioning etc etc can be applied

  - Volume Gateway (iSCSI) : This is a block based storage. It means we can upload datacenter, OS, databases to aws cloud. Data backed up as Snapshots and stored as EBS snapshots in AWS
    - Stored Volumes : All data will be present in local and it will be asynchronously copied to AWS. 1 GB to 16 TB is the data that can be stored.
    - Cached Volumes : All data will be stored in AWS S3 and leave a recent cache copy in local. 1GB to 32TB is data that can be stored.

  - Tape Gateway (TPL)/ Virtual Tape Library (VTL) : Used for backups and archiving.

Snowball :

. Before Snowball AWS has Import/export where huge data can be stored in local hard devices sent to Amazon for upload to AWS. Basically done because of slow internet.

. Snowball introduced where Amazon sends their own devices for data transfer.

. Snowball can import from S3 and export to S3.

. Three types:
   - Snowball : Just a storage device
   - Snowball Edge : Storage device also has compute capabilities. Can run lambda functions as well
   - Snowmobile : Very very huge amounts of data.

Static website using S3:

. Bucket name and Route 53 domain name shoukd match if we host static website with specific domain name.

. ****website url created by S3 will be in this format:
  http://BUCKETNAME.s3-website-AWSREGION.amazonaws.com
  
. ** Supports only http and not https

Logs:
----
1) Server Access Logs: In-depth details of requests made to bucket. Data stored in S3 with prefix.
2) Object Level Logging: Uses cloud trial to generate audit logs for requests made by api/sdk/console
3) Cloud Watch: S3 metrics

Notes:

Tag can be made to Bucket and also to individual objects. Tags assigned to Buckets will not be inherited to individual objects
Access for other AWS accounts can be given either at Bucket/object level using Permissions option
Multi part Upload: Uploading of data to S3 is easier if data is being uploaded in parts.
Query in Place: We can run queries to data stored in S3. Recuces cost and easy
S3 Select : Resource provided by S3 to run simple queries to retrive required data from S3 objects instead of pulling entire object.
Amazon Athena : Athena is a service used to analyze data stored in S3 using siple queries.
                Athena is serverless
RedShift Spectrum : Service provided by Amazon Redshift to run queries on exabytes of data.

Comparisions:

Class          Durability     Availability
S3 standard     99.99999..%    99.99%
S3 IA           99.9999...%    99.9%
S3 IA one zone  99.999...%     99.5%
Glacier         99.999....%    99.99%
RRS             99.99%         99.99%

